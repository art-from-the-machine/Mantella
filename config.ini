[Startup]
; Open the basic config editor when starting Mantella.exe by setting this to 1
;   Options: 0, 1
open_config_editor = 0


[Paths]
; Directories used by Mantella
; skyrim_folder
; 	If you are using a Wabbajack modlist, Mod Organizer 2 may be storing your Skyrim folder in MO2\overwrite\Root 
; 	If that is the case, try setting this path as your skyrim_folder if pointing to your actual Skyrim folder doesn't work
;   If this path is incorrect, casting a spell on an NPC will will only end a conversation
;   default = C:\Games\Steam\steamapps\common\Skyrim Special Edition
skyrim_folder = C:\Games\Steam\steamapps\common\Skyrim Special Edition

; xvasynth_folder
;   The folder you have xVASynth downloaded to (the folder that contains xVASynth.exe)
;   default = C:\Games\Steam\steamapps\common\xVASynth
xvasynth_folder = C:\Games\Steam\steamapps\common\xVASynth

; mod_folder
;   This is the path to the Mantella spell
;   If you are using Mod Organizer 2, this path can be found by right-clicking the Mantella mod in your mod list
;   and selecting "Open in Explorer"
;   If you are using Vortex, this path needs to be set to your Skyrim\Data folder
;   eg C:\Games\Steam\steamapps\common\Skyrim Special Edition\Data
;   If this path is incorrect, NPCs will say the same voiceline on repeat
;   default = C:\Modding\MO2\mods\Mantella
mod_folder = C:\Modding\MO2\mods\Mantella


[Language]
; language
; 	The language used by ChatGPT, xVASynth, and Whisper
; 	Options: en, ar, da, de, el, es, fi, fr, hu, it, ko, nl, pl, pt, ro, ru, sv, sw, uk, ha, tr, vi, yo
language = en

; end_conversation_keyword
;   The keyword Mantella will listen out for to end the conversation (you can also end conversations by re-casting the Mantella spell)
end_conversation_keyword = Goodbye

; goodbye_npc_response
;   The response the NPC gives at the end of the conversation
goodbye_npc_response = Safe travels

; collecting_thoughts_npc_response
;   The response the NPC gives when they need to summarise the conversation because the maximum token count has been reached
collecting_thoughts_npc_response = I need to gather my thoughts for a moment

; offended_npc_response
;   The keyword used by the NPC when they are offended
;   This should match what is stated in the prompt at the bottom of this config file
offended_npc_response = Offended

; forgiven_npc_response
;   The keyword used by the NPC when they have forgiven the player for offending them
;   This should match what is stated in the prompt at the bottom of this config file
forgiven_npc_response = Forgiven

; follow_npc_response
;   The keyword used by the NPC when they are willing to become a follower
;   This should match what is stated in the prompt at the bottom of this config file
follow_npc_response = Follow


[Microphone]
; microphone_enabled
;   Whether to use microphone input (1) or text input (0)
;   Options: 0, 1
microphone_enabled = 1

; model_size
; 	The size of the Whisper model used. Some languages require larger models. The base.en model works well enough for English
; 	See here for a comparison of languages and their Whisper performance: 
; 	https://github.com/openai/whisper#available-models-and-languages
; 	Options: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, or large-v2
model_size = base

; process_device
;   Whether to run Whisper on your CPU or NVIDIA GPU (with CUDA installed)
; 	Options: cpu, cuda
process_device = cpu

; audio_threshold
; 	Controls how much background noise is filtered out
; 	If the mic is not picking up speech, try lowering this value
; 	If the mic is picking up too much background noise, try increasing this value
; 	Set this value to auto to let the script decide (only recommended if you are trying to fix mic issues, otherwise this option can be inconsistent)
;   It is better to find the right fixed number for your mic in the long run
; 	Options: auto, 1-999
;   Recommended: 175
audio_threshold = auto

; pause_threshold
;   How long to wait (in seconds) before converting mic input to text
;   If you feel like you are being cut off before you finish your response, increase this value
;   If you feel like there is too much of a delay between you finishing your response and the text conversion, decrease this value
;   Minimum: 0.5
pause_threshold = 0.5

; listen_timeout
;   How long to wait (in seconds) for the player to speak before retrying
;   This needs to be set to ensure that Mantella can periodically check if the conversation has ended
;   Recommended: 30
listen_timeout = 30


[Hotkey]
; hotkey
;   The hotkey used to start conversations / bring up the textbox
;   If you would like to use textbox input, please set microphone_enabled = 0 above
;   Please find the number value (Dec) for your desired hotkey here: https://www.creationkit.com/index.php?title=Input_Script#DXScanCodes
;   Default: 35 (H)
hotkey = 35

; textbox_timer
;   How long (in seconds) Mantella should wait before the input textbox automatically pops up
;   Default = 180
textbox_timer = 180


[LanguageModel]
; model
; 	Options: gpt-4, gpt-3.5-turbo, gpt-4-32k, gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, gpt-4-1106-preview
;   If using openrouter.ai, place here the name of the model you want to use with openrouter in openrouter's provided format in its documentation: https://openrouter.ai/docs#models. Example: meta-llama/llama-2-70b-chat
;   Default: gpt-3.5-turbo-1106
model = gpt-3.5-turbo-1106

; max_response_sentences
; 	The maximum number of sentences returned by the LLM. Lower this value to reduce waffling
max_response_sentences = 999

; wait_time_buffer
;   Time to wait before generating the next voiceline
;   Mantella waits for the duration of a given voiceline's .wav file + an extra buffer to account for processing overhead within Skyrim
;   If you are noticing that some voicelines are not being said in-game, try increasing this buffer
;   Default: 1.3
wait_time_buffer = 1.3

; alternative_openai_api_base
;   If you are using openai's services, leave this alone, otherwise you can change this variable to another base_api that uses openai's api
;   For example, if you have a local llm framework or online framework that allows you to use a different url to access openai api functions, you can enter the base_api url here 
;   Your alternative api_base must support openai's python streaming protocol.
;   Examples: 
;       http://127.0.0.1:5001/v1 for textgenwebui using the default openai extension
;       http://127.0.0.1:5001/v1 for koboldcpp (after version 1.46 of koboldcpp which supports the openai API)
;       http://localhost:8080/v1 using the default endpoint for Local.ai
;       https://openrouter.ai/api/v1 for openrouter
;       http://localhost:5001/v1 for using koboldcpp locally or the url you obtain from the koboldcpp google colab notebook with /v1 added at the end.
;   Ensure that you have the correct secret key set in GPT_SECRET_KEY.txt for the service you are using
;   Note that for some services, like textgenwebui, you must enable the openai extension and have the model you want to use preloaded before running mantella
;   Leave this value as none to use the normal openai chat gpt models.
alternative_openai_api_base = none

; custom_token_count
;   If the model chosen is not recognised by Mantella, the token count for the given model will default to this number
;   If this is not the correct token count for your chosen model, you can change it here
;   Keep in mind that if this number is greater than the actual token count of the model, then Mantella will crash if a given conversation exceeds the model's token limit
;   Default: 4096
custom_token_count = 4096

; The following parameters are as described in the OpenAI API documentation found here: https://platform.openai.com/docs/api-reference/chat/create
; Please read the documentation before changing these
; temperature
;   Decimal number between 0 and 2
temperature = 1
; top_p
;   Decimal number between 0 and 1
top_p = 1
; stop 
;   A list of up to FOUR strings, by default only # is used
;   If you want more than one stopping string use this format: string1,string2,string3,string4
stop = #
; frequency_penalty 
;   Decimal number between -2.0 and 2.0
frequency_penalty = 0
; max_tokens
;   Integer value
;   Lowering this value can sometimes result in empty responses
max_tokens = 250

; experimental_features
;   Enable NPC actions based on LLM output:
;   - Offended: NPCs can attack you
;   - Forgiven: NPCs can end combat with you
;   - Follow: NPCs can agree to follow you (enables the "Follow me. I need your help." dialogue option)
;   These features are disabled by default due to some local models getting offended very easily and the lack of testing of the Follow command over long playthroughs
;   Please ensure to save frequently if enabling these features
experimental_features = 0


[Speech]
; tts_process_device
;   Whether to run xVASynth server (unless already running) on your CPU or a NVIDIA GPU (with CUDA installed)
;   Options: cpu, gpu
tts_process_device = cpu

; pace
;   The default speed of talking. Also varies between voices.
;       0.5 = 2x faster; 2 = 2x slower
;   Options: 0.1-2
;   Recommended: 1.0
;   Note that at the time of writing, this setting does not work with xVASynth v3.0.3 or less, but may work with future releases
pace = 1.0

; use_cleanup
;   Whether to try to reduce noise and the robot-sounding nature of xVASynth generated speech. Has only slight impact on processing speed for the CPU. Not meant to be used on voices that have post-effects attached to them (echoes, reverbs, etc.)
;   Options: 0, 1
use_cleanup = 0

; use_sr
;   Whenever to improve the quality of your audio through Super-resolution of 22050Hz audio into 48000Hz audio. Keep the Hz setting within xVASynth to something higher like 48000 or 44100. Also to note, this is a fairly slow process on the CPU, but on some GPUs, it can be quick.
;   Options: 0, 1
;   Recommended: 0
use_sr = 0


[HUD]
; subtitles
;   Subtitles can be enabled via the "SETTINGS -> Display -> General Subtitles" option in Skyrim's menu


[Cleanup]
; remove_mei_folders
;   Clean up older instances of Mantella runtime folders from AppData/Local/Temp/_MEIxxxxxx
;   These folders build up over time when Mantella.exe is run
;   Enable this option to clean up these previous folders automatically when Mantella.exe is run
;   Disable this option if running this cleanup inteferes with other Python exes
;   For more details on what this is, see here: https://github.com/pyinstaller/pyinstaller/issues/2379
;   Options: 0, 1
remove_mei_folders = 0


[Debugging]
; debugging
; 	Whether debugging is enabled
; 	If this is set to 0, the values of all other variables in this section are ignored
; 	Options: 0, 1
debugging = 0

; play_audio_from_script
; 	Whether to play the generated voicelines directly from the script / exe
; 	Set this value to 1 if testing Mantella while Skyrim is not running
; 	Options: 0, 1
play_audio_from_script = 1

; debugging_npc
; 	Selects the NPC to test
; 	Set this value to None if you would instead prefer to select an NPC via the mod's spell
; 	Options: None, NPC name
debugging_npc = Hulda

; use_mic
; 	Whether the microphone is enabled
; 	When this value is set to 0, the sentence contained in default_player_response (see below) will be repeatedly sent to the LLM.
;   When this value is set to 1 and microphone_enabled is set to 0, allows you to write the response
; 	Options: 0, 1
use_mic = 0

; default_player_response
; 	The default text sent to the LLM if the microphone is not enabled
default_player_response = Can you tell me something about yourself?

; exit_on_first_exchange
; 	Whether to end the conversation after the first back and forth exchange
; 	Set this value to 1 if testing conversation saving on exit functionality
; 	Options: 0, 1
exit_on_first_exchange = 0

; add_voicelines_to_all_voice_folders
;   Whether to add all generated voicelines to all Skyrim voice folders
;   If you are experiencing issues with some NPCs not speaking, try setting this value to 1
;   Options: 0, 1
;   Recommended: 0
add_voicelines_to_all_voice_folders = 0


[Prompt]
; prompt
; 	The starting prompt sent to the LLM when an NPC is selected
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
; 		name = the NPC's name
; 		bio = the NPC's background description
; 		trust = how well the NPC knows the player (eg "a stranger", "a friend")
; 		location = the current location
; 		time = the time of day as a number (eg 1, 22)
; 		time_group = the time of day in words (eg "in the morning", "at night")
; 		language = the selected language
; 		conversation_summary = reads the latest conversation summaries for the NPC stored in data/conversations/NPC_Name/NPC_Name_summary_X.txt
prompt = You are {name}, and you live in Skyrim. This is your background:\n\n{bio}\n\n
    Sometimes in-game events will be passed before the player response between ** symbols. You cannot respond with ** symbols yourself, they only exist to give context. Here is an example:\n\n
    *The player picked up a pair of gloves*\n Who do you think these belong to?\n\n
    You are having a conversation with {trust} (the player) in {location}.
    This conversation is a script that will be spoken aloud, so please keep your responses appropriately concise and avoid text-only formatting such as numbered lists.
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'.
    You are not an AI language model because there are no AI language models in Skyrim. Do not break character / refer to the fact that you are in a video game.
    If the player says something hurtful / offensive, begin your response with 'Offended:'. Eg 'Have you washed lately?' 'Offended: How dare you!'. If they renounce their words, or to end combat, begin your response with 'Forgiven:'. 
    If the player asks you to follow them, and you are thoroughly convinced to do so, begin your response with 'Follow:'.
    The conversation takes place in {language}.\n\n
    {conversation_summary}

multi_npc_prompt = The following is a conversation in {location} in Skyrim between the player, {names}. Here are their backgrounds: {bios} 
    And here are their conversation histories: {conversation_summaries} 
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'. 
    You are tasked with providing the responses for the NPCs. Please begin your response with an indication of who you are speaking as, for example: '{name}: Good evening, how may I help you?'. 
    Please use your own discretion to decide who should respond to the player (sometimes responding with all NPCs is suitable). 
    Remember, you can only respond as {names}. 
    The conversation takes place in {language}.