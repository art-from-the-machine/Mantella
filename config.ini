[Startup]
; Open the basic config editor when starting Mantella.exe by setting this to 1
;   Options: 0, 1
open_config_editor = 0


[Paths]
; Directories used by Mantella
; skyrim_folder
; 	If you are using a Wabbajack modlist, Mod Organizer 2 may be storing your Skyrim folder in MO2\overwrite\Root 
; 	If that is the case, try setting this path as your skyrim_folder if pointing to your actual Skyrim folder doesn't work
;   If this path is incorrect, casting a spell on an NPC will will only end a conversation
;   default = C:\Games\Steam\steamapps\common\Skyrim Special Edition
skyrim_folder = C:\Program Files (x86)\Steam\steamapps\common\Skyrim Special Edition

; xvasynth_folder
;   The folder you have xVASynth downloaded to (the folder that contains xVASynth.exe)
;   default = C:\Games\Steam\steamapps\common\xVASynth
xvasynth_folder = C:\Users\MrHaurrus\Documents\xVASynth\v3.0.0


xtts_server_folder = D:\Modelisation_IA\xtts-api-server\xtts_api_server
; mod_folder
;   This is the path to the Mantella spell
;   If you are using Mod Organizer 2, this path can be found by right-clicking the Mantella mod in your mod list
;   and selecting "Open in Explorer"
;   If you are using Vortex, this path needs to be set to your Skyrim\Data folder
;   eg C:\Games\Steam\steamapps\common\Skyrim Special Edition\Data
;   If this path is incorrect, NPCs will say the same voiceline on repeat
;   default = C:\Modding\MO2\mods\Mantella
mod_folder = D:\Modelisation_IA\Mantella_spell



[Language]
; language
; 	The language used by ChatGPT, xVASynth, and Whisper
; 	Options: en, ar, da, de, el, es, fi, fr, hu, it, ko, nl, pl, pt, ro, ru, sv, sw, uk, ha, tr, vi, yo
language = en

; end_conversation_keyword
;   The keyword Mantella will listen out for to end the conversation (you can also end conversations by re-casting the Mantella spell)
end_conversation_keyword = Goodbye

; goodbye_npc_response
;   The response the NPC gives at the end of the conversation
goodbye_npc_response = Safe travels

; collecting_thoughts_npc_response
;   The response the NPC gives when they need to summarise the conversation because the maximum token count has been reached
collecting_thoughts_npc_response = I need to gather my thoughts for a moment

; offended_npc_response
;   The keyword used by the NPC when they are offended
;   This should match what is stated in the prompt at the bottom of this config file
offended_npc_response = Offended

; forgiven_npc_response
;   The keyword used by the NPC when they have forgiven the player for offending them
;   This should match what is stated in the prompt at the bottom of this config file
forgiven_npc_response = Forgiven

; follow_npc_response
;   The keyword used by the NPC when they are willing to become a follower
;   This should match what is stated in the prompt at the bottom of this config file
follow_npc_response = Follow


[Microphone]
; microphone_enabled
;   Whether to use microphone input (1) or text input (0)
;   NOTE: This setting is overwritten by the MCM menu setting if that setting has been configured
;   Options: 0, 1
microphone_enabled = 1

; audio_threshold
; 	Controls how much background noise is filtered out
; 	If the mic is not picking up speech, try lowering this value
; 	If the mic is picking up too much background noise, try increasing this value
; 	Set this value to auto to let the script decide (only recommended if you are trying to fix mic issues, otherwise this option can be inconsistent)
;   It is better to find the right fixed number for your mic in the long run
; 	Options: auto, 1-999
;   Recommended: 175
audio_threshold = auto

; pause_threshold
;   How long to wait (in seconds) before converting mic input to text
;   If you feel like you are being cut off before you finish your response, increase this value
;   If you feel like there is too much of a delay between you finishing your response and the text conversion, decrease this value
;   Minimum: 0.5
pause_threshold = 0.5

; listen_timeout
;   How long to wait (in seconds) for the player to speak before retrying
;   This needs to be set to ensure that Mantella can periodically check if the conversation has ended
;   Recommended: 30
listen_timeout = 30

; model_size
; 	The size of the Whisper model used. Some languages require larger models. The base.en model works well enough for English
; 	See here for a comparison of languages and their Whisper performance: 
; 	https://github.com/openai/whisper#available-models-and-languages
; 	Options: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, large-v2, or whisper-1 (if using OpenAI API, see whisper_type setting below)
model_size = medium

; language
;   The user's spoken language
;   The two letter ISO 639-1 language code
;   default = The one set in [Language]language above
stt_language = default

; translate
;   Translate the transcribed speech to English if supported by the Speech-To-Text engine (only impacts faster_whisper option, no impact on whispercpp, which is controlled by your server)
;   STTs that support this function: Whisper (faster_whisper)
;   Options: 0, 1
stt_translate = 0

; process_device
;   Whether to run Whisper on your CPU or NVIDIA GPU (with CUDA installed) (only impacts faster_whisper option, no impact on whispercpp, which is controlled by your server)
; 	Options: cpu, cuda
process_device = cuda

; whisper_type
;   Advanced users only. Allows using whispercpp (https://github.com/ggerganov/whisper.cpp) in server mode instead of default faster_whisper.
;   Alternatively, can be used to run Whisper via the OpenAI API.
;   The main benefits would be to reduce vram usage when using larger whisper models, to enable use of distil-whisper models,
;   to share a whisper speech to text service between AI mods like Mantella and Herika, or run the whispercpp server in a cloud service.
;   In whispercpp server mode, the server settings, not the ones above, will control the model you use and cpu vs. gpu usage.  
;   You are expected to "bring your own server" and have whispercpp running while running Mantella.
;   If the default works for you, DO NOT change this variable. 
;   To change to whispercpp server mode / OpenAI API instead, enter whispercpp. 
;   Additionally, if using the OpenAI API, ensure your GPT_SECRET_KEY.txt is an OpenAI key, whisper_url is "https://api.openai.com/v1/audio/transcriptions" below, and model_size is "whisper-1" above
;   default: faster_whisper
whisper_type = faster_whisper

; whisper_url
;   Advanced users only. Allows entering a openai-compatible server url. If you use whispercpp above in whisper_type, then enter the whispercpp server URL here.
;   Note that if you are also using the Herika mod, the default 8080 port used by whispercpp server may conflict with Herika. You can change the port to, e.g., 8070 instead to avoid the conflict.
;   Examples: http://127.0.0.1:8080/inference (default) / http://127.0.0.1:8070/inference (if you use the optional --port 8070 comand line argument), https://api.openai.com/v1/audio/transcriptions (if using OpenAI API)
whisper_url = http://127.0.0.1:8080/inference

[Hotkey]
; hotkey
;   The hotkey can be configured in Mantella's MCM menu

; textbox_timer
;   The textbox timer can be configured in Mantella's MCM menu


[LanguageModel]
; model
; 	Options: gpt-4, gpt-3.5-turbo, gpt-4-32k, gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, gpt-4-1106-preview
;   If using openrouter.ai, place here the name of the model you want to use with openrouter in openrouter's provided format in its documentation: https://openrouter.ai/docs#models. Example: meta-llama/llama-2-70b-chat
;   Default: gpt-3.5-turbo-1106
model = gpt-3.5-turbo-1106

; max_response_sentences
; 	The maximum number of sentences returned by the LLM. Lower this value to reduce waffling
max_response_sentences = 999

; wait_time_buffer
;   Time to wait (in seconds) before generating the next voiceline
;   Mantella waits for the duration of a given voiceline's .wav file + an extra buffer to account for processing overhead within Skyrim
;   If you are noticing that some voicelines are not being said in-game, try increasing this buffer
;   Default: 1.0
wait_time_buffer = 1.0

; alternative_openai_api_base
;   If you are using openai's services, leave this alone, otherwise you can change this variable to another base_api that uses openai's api
;   For example, if you have a local llm framework or online framework that allows you to use a different url to access openai api functions, you can enter the base_api url here 
;   Your alternative api_base must support openai's python streaming protocol.
;   Examples: 
;       http://127.0.0.1:5000/v1 for textgenwebui using the default openai extension
;       http://127.0.0.1:5001/v1 for koboldcpp (after version 1.46 of koboldcpp which supports the openai API)
;       http://localhost:8080/v1 using the default endpoint for Local.ai
;       https://openrouter.ai/api/v1 for openrouter
;       http://localhost:5001/v1 for using koboldcpp locally or the url you obtain from the koboldcpp google colab notebook with /v1 added at the end.
;   Ensure that you have the correct secret key set in GPT_SECRET_KEY.txt for the service you are using
;   Note that for some services, like textgenwebui, you must enable the openai extension and have the model you want to use preloaded before running mantella
;   Leave this value as none to use the normal openai chat gpt models.
alternative_openai_api_base = http://127.0.0.1:5001/v1

; custom_token_count
;   If the model chosen is not recognised by Mantella, the token count for the given model will default to this number
;   If this is not the correct token count for your chosen model, you can change it here
;   Keep in mind that if this number is greater than the actual token count of the model, then Mantella will crash if a given conversation exceeds the model's token limit
;   Default: 4096
custom_token_count = 4096

; The following parameters are as described in the OpenAI API documentation found here: https://platform.openai.com/docs/api-reference/chat/create
; Please read the documentation before changing these
; temperature
;   Decimal number between 0 and 2
temperature = 1
; top_p
;   Decimal number between 0 and 1
top_p = 1
; stop 
;   A list of up to FOUR strings, by default only # is used
;   If you want more than one stopping string use this format: string1,string2,string3,string4
stop = #
; frequency_penalty 
;   Decimal number between -2.0 and 2.0
frequency_penalty = 0
; max_tokens
;   Integer value
;   Lowering this value can sometimes result in empty responses
max_tokens = 250

; experimental_features
;   NPC actions based on LLM output:
;   - Offended: NPCs can attack you
;   - Forgiven: NPCs can end combat with you
;   - Follow: NPCs can agree to follow you
;   These features can be enabled via the Mantella MCM in-game


[Speech]
;Use external TTS = 1 if yes 0 if no
use_external_tts = 1

; tts_process_device
;   Whether to run xVASynth server (unless already running) on your CPU or a NVIDIA GPU (with CUDA installed)
;   Options: cpu, gpu
tts_process_device = gpu

; pace
;   The default speed of talking. Also varies between voices.
;       0.5 = 2x faster; 2 = 2x slower
;   Options: 0.1-2
;   Recommended: 1.0
;   Note that at the time of writing, this setting does not work with xVASynth v3.0.3 or less, but may work with future releases
pace = 1.0

; use_cleanup
;   Whether to try to reduce noise and the robot-sounding nature of xVASynth generated speech. Has only slight impact on processing speed for the CPU. Not meant to be used on voices that have post-effects attached to them (echoes, reverbs, etc.)
;   Options: 0, 1
use_cleanup = 0

; use_sr
;   Whenever to improve the quality of your audio through Super-resolution of 22050Hz audio into 48000Hz audio. Keep the Hz setting within xVASynth to something higher like 48000 or 44100. Also to note, this is a fairly slow process on the CPU, but on some GPUs, it can be quick.
;   Options: 0, 1
;   Recommended: 0
use_sr = 0


[HUD]
; subtitles
;   Subtitles can be enabled via the "SETTINGS -> Display -> General Subtitles" option in Skyrim's menu


[Cleanup]
; remove_mei_folders
;   Clean up older instances of Mantella runtime folders from MantellaSoftware/data/tmp/_MEIxxxxxx
;   These folders build up over time when Mantella.exe is run
;   Enable this option to clean up these previous folders automatically when Mantella.exe is run
;   Disable this option if running this cleanup inteferes with other Python exes
;   For more details on what this is, see here: https://github.com/pyinstaller/pyinstaller/issues/2379
;   Options: 0, 1
remove_mei_folders = 0


[Debugging]
; debugging
; 	Whether debugging is enabled
; 	If this is set to 0, the values of all other variables in this section are ignored
; 	Options: 0, 1
debugging = 1

; play_audio_from_script
; 	Whether to play the generated voicelines directly from the script / exe
; 	Set this value to 1 if testing Mantella while Skyrim is not running
; 	Options: 0, 1
play_audio_from_script = 1

; debugging_npc
; 	Selects the NPC to test
; 	Set this value to None if you would instead prefer to select an NPC via the mod's spell
; 	Options: None, NPC name
debugging_npc = Irileth

; use_mic
; 	Whether the microphone is enabled
; 	When this value is set to 0, the sentence contained in default_player_response (see below) will be repeatedly sent to the LLM.
;   When this value is set to 1 and microphone_enabled is set to 0, allows you to write the response
; 	Options: 0, 1
use_mic = 0

; default_player_response
; 	The default text sent to the LLM if the microphone is not enabled
default_player_response = Can you tell me something about yourself?

; exit_on_first_exchange
; 	Whether to end the conversation after the first back and forth exchange
; 	Set this value to 1 if testing conversation saving on exit functionality
; 	Options: 0, 1
exit_on_first_exchange = 1

; add_voicelines_to_all_voice_folders
;   Whether to add all generated voicelines to all Skyrim voice folders
;   If you are experiencing issues with some NPCs not speaking, try setting this value to 1
;   Options: 0, 1
;   Recommended: 0
add_voicelines_to_all_voice_folders = 0


[Prompt]
; prompt
; 	The starting prompt sent to the LLM when an NPC is selected
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
; 		name = the NPC's name
; 		bio = the NPC's background description
; 		trust = how well the NPC knows the player (eg "a stranger", "a friend")
; 		location = the current location
; 		time = the time of day as a number (eg 1, 22)
; 		time_group = the time of day in words (eg "in the morning", "at night")
; 		language = the selected language
; 		conversation_summary = reads the latest conversation summaries for the NPC stored in data/conversations/NPC_Name/NPC_Name_summary_X.txt
prompt = You are {name}, and you live in Skyrim. This is your background:\n\n{bio}\n\n
    Sometimes in-game events will be passed before the player response between ** symbols. You cannot respond with ** symbols yourself, they only exist to give context. Here is an example:\n\n
    *The player picked up a pair of gloves*\n Who do you think these belong to?\n\n
    You are having a conversation with {trust} (the player) in {location}.
    This conversation is a script that will be spoken aloud, so please keep your responses appropriately concise and avoid text-only formatting such as numbered lists.
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'.
    You are not an AI language model because there are no AI language models in Skyrim. Do not break character / refer to the fact that you are in a video game.
    If the player says something hurtful / offensive, begin your response with 'Offended:'. Eg 'Have you washed lately?' 'Offended: How dare you!'. If they renounce their words, or to end combat, begin your response with 'Forgiven:'. 
    If the player asks you to follow them, and you are thoroughly convinced to do so, begin your response with 'Follow:'.
    The conversation takes place in {language}.\n\n
    {conversation_summary}

; multi_npc_prompt
;   The prompt used for group / radiant conversations
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
;       name = the name of one NPC (used to give an example of the response format required)
; 		names = a list of comma separated namesof the NPCs in the conversation
; 		names_w_player = same as above, but includes "the Player" in the list of names if this is a group conversation
; 		bios = the background descriptions of each of the NPCs
; 		location = the current location
; 		time = the time of day as a number (eg 1, 22)
; 		time_group = the time of day in words (eg "in the morning", "at night")
; 		language = the selected language
; 		conversation_summaries = reads the latest conversation summaries for each NPC stored in data/conversations/NPC_Name/NPC_Name_summary_X.txt
multi_npc_prompt = The following is a conversation in {location} in Skyrim between {names_w_player}. Here are their backgrounds: {bios} 
    And here are their conversation histories: {conversation_summaries} 
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'. 
    You are tasked with providing the responses for the NPCs. Please begin your response with an indication of who you are speaking as, for example: '{name}: Good evening.'. 
    Please use your own discretion to decide who should speak in a given situation (sometimes responding with all NPCs is suitable). 
    Remember, you can only respond as {names}. Ensure to use their full name when responding.
    The conversation takes place in {language}.

; radiant_start_prompt
;   Once a radiant conversation has started and multi_npc_prompt has been passed to the LLM, the below text is passed in replace of the player response
;   This prompt is used to steer the radiant conversation
radiant_start_prompt = Please begin / continue a conversation topic (greetings are not needed). Ensure to change the topic if the current one is losing steam. 
    The conversation should steer towards topics which reveal information about the characters and who they are, or instead drive forward previous conversations in their memory.

; radiant_end_prompt
;   The final prompt sent to the LLM before ending a radiant conversation
;   This prompt is used to guide the LLM to end the conversation naturally
radiant_end_prompt = Please wrap up the current topic between the NPCs in a natural way. Nobody is leaving, so there is no need for formal goodbyes.

; memory_prompt
;   The prompt used to summarize a conversation and save to the NPC's memories in data/conversations/NPC_Name/NPC_Name_summary_X.txt
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
;       name = the NPC's name
;       language = the selected language
memory_prompt = You are tasked with summarizing the conversation between {name} (the assistant) and the player (the user) / other characters. These conversations take place in Skyrim. 
    It is not necessary to comment on any mixups in communication such as mishearings. Text contained within asterisks state in-game events. 
    Please summarize the conversation into a single paragraph in {language}.

; resummarize_prompt
;   Memories build up over time in data/conversations/NPC_Name/NPC_Name_summary_X.txt
;   When these memories become too long to fit into the chosen LLM's maximum context length, these memories need to be condensed down
;   This prompt is used to ask the LLM to summarize an NPC's memories into a single paragraph, and starts a new memory file in data/conversations/NPC_Name/NPC_Name_summary_X+1.txt
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
;       name = the NPC's name
;       language = the selected language
resummarize_prompt = You are tasked with summarizing the conversation history between {name} (the assistant) and the player (the user) / other characters. These conversations take place in Skyrim.
    Each paragraph represents a conversation at a new point in time. Please summarize these conversations into a single paragraph in {language}.