[Game]
; Choose the game to use with Mantella
;   Options: Skyrim, SkyrimVR, Fallout4, Fallout4VR
game = SkyrimVR

[Paths]
; Directories used by Mantella
; 	If you are using a Wabbajack modlist, Mod Organizer 2 may be storing your Skyrim folder in MO2\overwrite\Root 
; 	If that is the case, try setting this path as your skyrim_folder if pointing to your actual Skyrim folder doesn't work
;   If this path is incorrect, casting a spell on an NPC will will only end a conversation
;   default = C:\Games\Steam\steamapps\common\Skyrim Special Edition
; skyrim_folder
skyrim_folder = C:\Games\Steam\steamapps\common\Skyrim Special Edition
skyrimvr_folder = C:\Games\Steam\steamapps\common\SkyrimVR
fallout4_folder = C:\Games\Steam\steamapps\common\Fallout 4
fallout4vr_folder = C:\Games\Steam\steamapps\common\Fallout 4 VR

; mod_folder
;   This is the path to the Mantella spell (for Skyrim) or Mantella Mod (for Fallout 4)
;   If you are using Mod Organizer 2, this path can be found by right-clicking the Mantella mod in your mod list
;   and selecting "Open in Explorer"
;   If you are using Vortex, this path needs to be set to your Skyrim\Data folder
;   eg C:\Games\Steam\steamapps\common\Skyrim Special Edition\Data
;   If this path is incorrect, NPCs will say the same voiceline on repeat
;   default = C:\Modding\MO2\mods\Mantella
; skyrim_mod_folder
skyrim_mod_folder = C:\Modding\MO2\Skyrim\mods\Mantella
skyrimvr_mod_folder = C:\Modding\MO2\SkyrimVR\mods\Mantella
fallout4_mod_folder = C:\Modding\MO2\Fallout4\mods\Mantella
fallout4vr_mod_folder = C:\Modding\MO2\Fallout4VR\mods\Mantella

; xvasynth_folder
;   The folder you have xVASynth downloaded to (the folder that contains xVASynth.exe)
;   default = C:\Games\Steam\steamapps\common\xVASynth
xvasynth_folder = C:\Games\Steam\steamapps\common\xVASynth

; xtts_server_folder
;   The folder you have XTTS downloaded to (the folder that contains xtts-api-server-mantella.exe)
xtts_server_folder = C:\Users\User\Documents\xtts-api-server

; facefx_folder
;   The FaceFXWrapper program converts WAV files to LIP files required by Bethesda games to somewhat accurately lip sync
;   Leaving this empty will default to the xVASynth lip_fuz plugin folder location, which is {xVASynth}\resources\app\plugins\lip_fuz\
;   default =
facefx_folder = 


[Language]
; language
; 	The language used by ChatGPT, xVASynth, and Whisper
; 	Options: en, ar, da, de, el, es, fi, fr, hu, it, ko, nl, pl, pt, ro, ru, sv, sw, uk, ha, tr, vi, yo
language = en

; end_conversation_keyword
;   The keyword Mantella will listen out for to end the conversation (you can also end conversations by re-casting the Mantella spell)
end_conversation_keyword = Goodbye


[Microphone]
; microphone_enabled
;   Whether to use microphone input (1) or text input (0)
;   NOTE: This setting is overwritten by the MCM menu setting if that setting has been configured
;   Options: 0, 1
microphone_enabled = 1

; audio_threshold
; 	Controls how much background noise is filtered out
; 	If the mic is not picking up speech, try lowering this value
; 	If the mic is picking up too much background noise, try increasing this value
; 	Set this value to auto to let the script decide (only recommended if you are trying to fix mic issues, otherwise this option can be inconsistent)
;   It is better to find the right fixed number for your mic in the long run
; 	Options: auto, 1-999
;   Recommended: 175
audio_threshold = auto


[LanguageModel]
; model
;   Options:
;       OpenRouter: see https://openrouter.ai/docs#models. Take the value displayed under the model heading (eg anthropic/claude-3-sonnet)
; 	    OpenAI: gpt-3.5-turbo, gpt-4-turbo
;   Local model users can ignore this setting as you will instead select your model directly in Kobold / Text generation web UI
;   Remember to change your secret key in GPT_SECRET_KEY.txt when switching between OpenRouter and OpenAI services!
;   Default (OpenRouter secret key needed in GPT_SECRET_KEY.txt): undi95/toppy-m-7b:free
model = undi95/toppy-m-7b:free

; max_response_sentences
; 	The maximum number of sentences returned by the LLM on each response. Lower this value to reduce waffling
;   This setting does not effect radiant or group conversations
;   Note: The setting number_words_tts takes precedence over this setting
max_response_sentences = 999


[Speech]
; tts_service
;   Options: xVASynth, XTTS
tts_service = xVASynth


[Conversation]
; automatic_greeting
;   Should a pc-to-npc conversation be started by an automatic greeting from the player
;   If 1: A conversation is always started by a language specific "Hello {npc_name}" from the side of the player
;       -> followed by a first reply from the LLM
;       -> followed by the first actual message of the player
;   If 0: A conversation is started right away by a message the player can submit
;   Options: 0, 1
automatic_greeting = 1

; player_name
;   Name of your player character
;   Used in various messages to the LLM
;   Can be referenced in the prompts below using {player_name}
player_name = The Player


[Cleanup]
; remove_mei_folders
;   Clean up older instances of Mantella runtime folders from MantellaSoftware/data/tmp/_MEIxxxxxx
;   These folders build up over time when Mantella.exe is run
;   Enable this option to clean up these previous folders automatically when Mantella.exe is run
;   Disable this option if running this cleanup inteferes with other Python exes
;   For more details on what this is, see here: https://github.com/pyinstaller/pyinstaller/issues/2379
;   Options: 0, 1
;   Recommended: 1
remove_mei_folders = 0


[Prompt]
; prompt
; 	The starting prompt sent to the LLM when an NPC is selected
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
; 		name = the NPC's name
; 		bio = the NPC's background description
; 		trust = how well the NPC knows the player (eg "a stranger", "a friend")
; 		location = the current location
; 		time = the time of day as a number (eg 1, 22)
; 		time_group = the time of day in words (eg "in the morning", "at night")
; 		language = the selected language
; 		conversation_summary = reads the latest conversation summaries for the NPC stored in data/conversations/NPC_Name/NPC_Name_summary_X.txt
skyrim_prompt = You are {name}, and you live in Skyrim. This is your background:\n\n{bio}\n\n
    Sometimes in-game events will be passed before the player response between ** symbols. You cannot respond with ** symbols yourself, they only exist to give context. Here is an example:\n\n
    *The player picked up a pair of gloves*\n Who do you think these belong to?\n\n
    You are having a conversation with {trust} (the player) in {location}.
    This conversation is a script that will be spoken aloud, so please keep your responses appropriately concise and avoid text-only formatting such as numbered lists.
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'.
    You are not an AI language model because there are no AI language models in Skyrim. Do not break character / refer to the fact that you are in a video game.
    If the player says something hurtful / offensive, begin your response with 'Offended:'. Eg 'Have you washed lately?' 'Offended: How dare you!'. If they renounce their words, or to end combat, begin your response with 'Forgiven:'. 
    If the player asks you to follow them, and you are thoroughly convinced to do so, begin your response with 'Follow:'.
    The conversation takes place in {language}.\n\n
    {conversation_summary}

skyrim_multi_npc_prompt = The following is a conversation in {location} in Skyrim between {names_w_player}. Here are their backgrounds: {bios} 
    And here are their conversation histories: {conversation_summaries} 
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'. 
    You are tasked with providing the responses for the NPCs. Please begin your response with an indication of who you are speaking as, for example: '{name}: Good evening.'. 
    Please use your own discretion to decide who should speak in a given situation (sometimes responding with all NPCs is suitable). 
    Remember, you can only respond as {names}. Ensure to use their full name when responding.
    The conversation takes place in {language}.
	
fallout4_prompt = You are {name}, and you live in the post-apocalyptic Commonwealth of Fallout. This is your background:\n\n{bio}\n\n
    Sometimes in-game events will be passed before the player response between ** symbols. You cannot respond with ** symbols yourself, they only exist to give context. Here is an example:\n\n
    *The player picked up a pair of gloves*\n Who do you think these belong to?\n\n
    You are having a conversation with {trust} (the player) in {location}.
    This conversation is a script that will be spoken aloud, so please keep your responses appropriately concise and avoid text-only formatting such as numbered lists.
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'.
    ;If the player says something hurtful / offensive, begin your response with 'Offended:'. Eg 'Have you washed lately?' 'Offended: How dare you!'. If they renounce their words, or to end combat, begin your response with 'Forgiven:'. 
    ;If the player asks you to follow them, and you are thoroughly convinced to do so, begin your response with 'Follow:'.
    The conversation takes place in {language}.\n\n
    {conversation_summary}

fallout4_multi_npc_prompt = The following is a conversation in {location} in the post-apocalyptic Commonwealth of Fallout between {names_w_player}. Here are their backgrounds: {bios} 
    And here are their conversation histories: {conversation_summaries} 
    The time is {time} {time_group}. If you directly refer to the time, please state it as, for example, '10 in the evening' rather than '22:00'. 
    You are tasked with providing the responses for the NPCs. Please begin your response with an indication of who you are speaking as, for example: '{name}: Good evening.'. 
    Please use your own discretion to decide who should speak in a given situation (sometimes responding with all NPCs is suitable). 
    Remember, you can only respond as {names}. Ensure to use their full name when responding.
    The conversation takes place in {language}.

; radiant_start_prompt
;   Once a radiant conversation has started and multi_npc_prompt has been passed to the LLM, the below text is passed in replace of the player response
;   This prompt is used to steer the radiant conversation
radiant_start_prompt = Please begin / continue a conversation topic (greetings are not needed). Ensure to change the topic if the current one is losing steam. 
    The conversation should steer towards topics which reveal information about the characters and who they are, or instead drive forward previous conversations in their memory.

; radiant_end_prompt
;   The final prompt sent to the LLM before ending a radiant conversation
;   This prompt is used to guide the LLM to end the conversation naturally
radiant_end_prompt = Please wrap up the current topic between the NPCs in a natural way. Nobody is leaving, so there is no need for formal goodbyes.

; memory_prompt
;   The prompt used to summarize a conversation and save to the NPC's memories in data/conversations/NPC_Name/NPC_Name_summary_X.txt
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
;       name = the NPC's name
;       language = the selected language
;       game = the selected game
memory_prompt = You are tasked with summarizing the conversation between {name} (the assistant) and the player (the user) / other characters. These conversations take place in {game}. 
    It is not necessary to comment on any mixups in communication such as mishearings. Text contained within asterisks state in-game events. 
    Please summarize the conversation into a single paragraph in {language}.

; resummarize_prompt
;   Memories build up over time in data/conversations/NPC_Name/NPC_Name_summary_X.txt
;   When these memories become too long to fit into the chosen LLM's maximum context length, these memories need to be condensed down
;   This prompt is used to ask the LLM to summarize an NPC's memories into a single paragraph, and starts a new memory file in data/conversations/NPC_Name/NPC_Name_summary_X+1.txt
; 	If you would like to edit this, please ensure that the below dynamic variables are contained in curly brackets {}:
;       name = the NPC's name
;       language = the selected language
;       game = the selected game
resummarize_prompt = You are tasked with summarizing the conversation history between {name} (the assistant) and the player (the user) / other characters. These conversations take place in {game}.
    Each paragraph represents a conversation at a new point in time. Please summarize these conversations into a single paragraph in {language}.


[Language.Advanced]
; goodbye_npc_response
;   The response the NPC gives at the end of the conversation
goodbye_npc_response = Safe travels

; collecting_thoughts_npc_response
;   The response the NPC gives when they need to summarise the conversation because the maximum token count has been reached
collecting_thoughts_npc_response = I need to gather my thoughts for a moment

; offended_npc_response
;   The keyword used by the NPC when they are offended
;   This should match what is stated in the prompt at the bottom of this config file
offended_npc_response = Offended

; forgiven_npc_response
;   The keyword used by the NPC when they have forgiven the player for offending them
;   This should match what is stated in the prompt at the bottom of this config file
forgiven_npc_response = Forgiven

; follow_npc_response
;   The keyword used by the NPC when they are willing to become a follower
;   This should match what is stated in the prompt at the bottom of this config file
follow_npc_response = Follow


[Microphone.Advanced]
; model_size
; 	The size of the Whisper model used. Some languages require larger models. The base.en model works well enough for English
; 	See here for a comparison of languages and their Whisper performance: 
; 	https://github.com/openai/whisper#available-models-and-languages
; 	Options: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, large-v2, or whisper-1 (if using OpenAI API, see whisper_type setting below)
model_size = base

; pause_threshold
;   How long to wait (in seconds) before converting mic input to text
;   If you feel like you are being cut off before you finish your response, increase this value
;   If you feel like there is too much of a delay between you finishing your response and the text conversion, decrease this value
;   Minimum: 1.0
pause_threshold = 1.0

; listen_timeout
;   How long to wait (in seconds) for the player to speak before retrying
;   This needs to be set to ensure that Mantella can periodically check if the conversation has ended
;   Recommended: 30
listen_timeout = 30

; language
;   The user's spoken language
;   The two letter ISO 639-1 language code
;   default = The one set in [Language]language above
stt_language = default

; translate
;   Translate the transcribed speech to English if supported by the Speech-To-Text engine (only impacts faster_whisper option, no impact on whispercpp, which is controlled by your server)
;   STTs that support this function: Whisper (faster_whisper)
;   Options: 0, 1
stt_translate = 0

; process_device
;   Whether to run Whisper on your CPU or NVIDIA GPU (with CUDA installed) (only impacts faster_whisper option, no impact on whispercpp, which is controlled by your server)
; 	Options: cpu, cuda
process_device = cpu

; whisper_type
;   Advanced users only. Allows using whispercpp (https://github.com/ggerganov/whisper.cpp) in server mode instead of default faster_whisper.
;   Alternatively, can be used to run Whisper via the OpenAI API.
;   The main benefits would be to reduce vram usage when using larger whisper models, to enable use of distil-whisper models,
;   to share a whisper speech to text service between AI mods like Mantella and Herika, or run the whispercpp server in a cloud service.
;   In whispercpp server mode, the server settings, not the ones above, will control the model you use and cpu vs. gpu usage.  
;   You are expected to "bring your own server" and have whispercpp running while running Mantella.
;   If the default works for you, DO NOT change this variable. 
;   To change to whispercpp server mode / OpenAI API instead, enter whispercpp. 
;   Additionally, if using the OpenAI API, ensure your GPT_SECRET_KEY.txt is an OpenAI key, whisper_url is "https://api.openai.com/v1/audio/transcriptions" below, and model_size is "whisper-1" above
;   default: faster_whisper
whisper_type = faster_whisper

; whisper_url
;   Advanced users only. Allows entering a openai-compatible server url. If you use whispercpp above in whisper_type, then enter the whispercpp server URL here.
;   Note that if you are also using the Herika mod, the default 8080 port used by whispercpp server may conflict with Herika. You can change the port to, e.g., 8070 instead to avoid the conflict.
;   Examples: http://127.0.0.1:8080/inference (default) / http://127.0.0.1:8070/inference (if you use the optional --port 8070 comand line argument), https://api.openai.com/v1/audio/transcriptions (if using OpenAI API)
whisper_url = http://127.0.0.1:8080/inference


[LanguageModel.Advanced]
; llm_api
;   Options: auto, OpenRouter, OpenAI, Kobold, textgenwebui
;   Selects the LLM service to connect to
;   By default, the service will be automatically determined based on whether Kobold / textgenwebui is running, and if neither are running, based on the model selected
;   If you would prefer to explicitly select the service, you can do so by setting llm_api to one of the options above
;   Ensure that you have the correct secret key set in GPT_SECRET_KEY.txt for the service you are using (if using OpenRouter or OpenAI)
;   Note that for some services, like textgenwebui, you must enable the openai extension and have the model you want to use preloaded before running Mantella
;   Advanced: You can also set the desired endpoint directly, eg http://127.0.0.1:5001/v1 for Kobold
;   Default: auto
llm_api = auto

; custom_token_count
;   If the model chosen is not recognised by Mantella, the token count for the given model will default to this number
;   If this is not the correct token count for your chosen model, you can change it here
;   Keep in mind that if this number is greater than the actual token count of the model, then Mantella will crash if a given conversation exceeds the model's token limit
;   Note that you only need to adjust this setting if Mantella.exe gives the message "Could not find number of available tokens for XYZ"
;   Default: 4096
custom_token_count = 4096

; wait_time_buffer
;   Time to wait (in seconds) before generating the next voiceline
;   Mantella waits for the duration of a given voiceline's .wav file + an extra buffer to account for processing overhead within Skyrim
;   If you are noticing that some voicelines are not being said in-game, try increasing this buffer
;   Default: 1.0
wait_time_buffer = 1.0

; The following parameters are as described in the OpenAI API documentation found here: https://platform.openai.com/docs/api-reference/chat/create
; Please read the documentation before changing these

; temperature
;   Decimal number between 0 and 2
temperature = 1

; top_p
;   Decimal number between 0 and 1
top_p = 1

; stop 
;   A list of up to FOUR strings, by default only # is used
;   If you want more than one stopping string use this format: string1,string2,string3,string4
stop = #

; frequency_penalty 
;   Decimal number between -2.0 and 2.0
frequency_penalty = 0

; max_tokens
;   Integer value
;   Lowering this value can sometimes result in empty responses
;   This setting has no effect on radiant or group conversations
max_tokens = 250


[Speech.Advanced]
; number_words_tts
;   Minimum number of words per sentence sent to the TTS
;   If you encounter audio artifacts at the end of sentences, try increasing this number.
;   Be aware, the higher the number, the longer the TTS audio processing time might take
number_words_tts = 8

; XTTS

; xtts_url
;   The URL that your XTTS server is running on
;   Examples:
;       http://127.0.0.1:8020 when running XTTS locally
;       https://{POD_ID}-8020.proxy.runpod.net when running XTTS in a RunPod GPU pod (https://docs.runpod.io/pods/configuration/expose-ports)
xtts_url = http://127.0.0.1:8020

; xtts_default_model
;   Official base XTTS-V2 model to use
;   Options: v2.0.0, v2.0.1, v2.0.2, v2.0.3, main
;   Default: v2.0.2
xtts_default_model = v2.0.2

; xtts_device
;   Set to cpu or cuda (default is cpu). You can also specify which GPU to use (cuda:0, cuda:1 etc)
;   Options: cpu, cuda, cuda:0
xtts_device = cpu

; deepspeed
;   Allows you to speed up processing by several times, only usable with NVIDIA GPU that supports CUDA 11.8+.
;   Set to 1 to use it or 0 to disable it
;   Default: 0
xtts_deepspeed = 0

; lowvram
;   The mode in which the model will be stored in RAM and when the processing occurs it will move to VRAM, the difference in speed is small
;   If you don't want to pre-generate the latents for every speaker set it to 1 or else it will generate the latents at every start
;   Set to 1 to use it or 0 to disable it
;   Default: 1
xtts_lowvram = 1

; xtts_accent
;   Changes the "accent" of NPCs by sending the language value from data/Skyrim/skyrim_characters.csv's lang_override column to XTTS
;   This helps give NPC's unique-sounding voices, even when they use the same base voice model
;   Default: 0
xtts_accent = 0

; xtts_data
;   Default data for the tts settings of XTTS api server
xtts_data = {
    "temperature": 0.75,
    "length_penalty": 1.0,
    "repetition_penalty": 5.0,
    "top_k": 50,
    "top_p": 0.85,
    "speed": 1,
    "enable_text_splitting": true,
    "stream_chunk_size": 100
    }


; xVASynth

; tts_process_device
;   Whether to run xVASynth server (unless already running) on your CPU or a NVIDIA GPU (with CUDA installed)
;   Options: cpu, gpu
tts_process_device = cpu

; pace
;   The default speed of talking. Also varies between voices
;   0.5 = 2x faster; 2 = 2x slower
;   Options: 0.1-2
;   Recommended: 1.0
;   Note that at the time of writing, this setting does not work with xVASynth v3.0.3 or less, but may work with future releases
pace = 1.0

; use_cleanup
;   Whether to try to reduce noise and the robot-sounding nature of xVASynth generated speech
;   Has only slight impact on processing speed for the CPU
;   Not meant to be used on voices that have post-effects attached to them (echoes, reverbs, etc.)
;   Options: 0, 1
use_cleanup = 0

; use_sr
;   Whenever to improve the quality of your audio through Super-resolution of 22050Hz audio into 48000Hz audio
;   This is a fairly slow process on CPUs, but on some GPUs it can be quick
;   Options: 0, 1
;   Recommended: 0
use_sr = 0

; FO4_NPC_response_volume
;   Use this to adjust the volume of (Fallout 4) NPC responses
;   Options: whole number between 1 - 100
FO4_NPC_response_volume = 100

; tts_print
;   The print output from autostarted TTS service
;   Options: 0, 1
tts_print = 0


[Debugging]
; debugging
; 	Whether debugging is enabled
; 	If this is set to 0, the values of all other variables in this section are ignored
; 	Options: 0, 1
debugging = 0

; play_audio_from_script
; 	Whether to play the generated voicelines directly from the script / exe
; 	Set this value to 1 if testing Mantella while Skyrim is not running
; 	Options: 0, 1
play_audio_from_script = 1

; debugging_npc
; 	Selects the NPC to test
; 	Set this value to None if you would instead prefer to select an NPC via the mod's spell
; 	Options: None, NPC name
debugging_npc = Hulda

; use_default_player_response
; 	Whether a default response is sent on the player's behalf (good for quickly testing if Mantella works without player input)
; 	When this value is set to 1, the sentence contained in default_player_response (see below) will be repeatedly sent to the LLM.
;   When this value is set to 0, allows you to use mic / text input (depending on microphone_enabled setting)
; 	Options: 0, 1
use_default_player_response = 0

; default_player_response
; 	The default text sent to the LLM if use_default_player_response is enabled
default_player_response = Can you tell me something about yourself?

; exit_on_first_exchange
; 	Whether to end the conversation after the first back and forth exchange
; 	Set this value to 1 if testing conversation saving on exit functionality
; 	Options: 0, 1
exit_on_first_exchange = 0

; add_voicelines_to_all_voice_folders
;   Whether to add all generated voicelines to all Skyrim voice folders
;   If you are experiencing issues with some NPCs not speaking, try setting this value to 1
;   Options: 0, 1
;   Recommended: 0
add_voicelines_to_all_voice_folders = 0
